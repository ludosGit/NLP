{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import torch\n",
    "import torchtext\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: using counter and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n",
      "(2, 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all')\n",
      "[121, 5501, 31, 5, 867, 11, 27, 875, 138, 2933, 5734, 103, 50]\n"
     ]
    }
   ],
   "source": [
    "vocab = None\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "def load_dataset(ngrams=1,min_freq=1):\n",
    "    global vocab, tokenizer\n",
    "    print(\"Loading dataset...\")\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    y_train = train_df['target']\n",
    "    x_train = train_df[['text', 'target']]\n",
    "    x_test = test_df['text']\n",
    "    classes = [i+1 for i in set(y_train)]\n",
    "    print('Building vocab...')\n",
    "    counter = collections.Counter()\n",
    "    # simply counts the different words in the dataset\n",
    "    for line in x_train['text']:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=ngrams))\n",
    "    vocab = torchtext.vocab.Vocab(counter, min_freq=min_freq)\n",
    "    x_train = [(x_train[\"target\"][i]+1, x_train[\"text\"][i]) for i in range(len(x_train))]\n",
    "    return x_train,x_test,classes,vocab\n",
    "\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "print(next(iter(train_dataset)))\n",
    "\n",
    "def encode(x,voc=None,unk=0,tokenizer=tokenizer):\n",
    "    v = vocab if voc is None else voc\n",
    "    return [v.stoi.get(s,unk) for s in tokenizer(x)]\n",
    "\n",
    "print(encode(train_dataset[0][1]))\n",
    "# encode maps the words to the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23547"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocab size = 96729\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "bi_counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    bi_counter.update(ngrams_iterator(tokenizer(line),ngrams=2))\n",
    "bi_vocab = torchtext.vocab.Vocab(bi_counter, min_freq=1)\n",
    "\n",
    "print(f\"Bigram vocab size = {len(bi_vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text:\n",
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "\n",
      "BoW vector:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "[23547, 23547]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(f\"sample text:\\n{train_dataset[0][1]}\")\n",
    "print(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")\n",
    "print([len(to_bow(train_dataset[0][1])), vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# BoW with term frequency - inverse document frequency to account the most - frequent / stop words\n",
    "N = 1000\n",
    "df = torch.zeros(vocab_size)\n",
    "for _,line in train_dataset[:N]:\n",
    "    for i in set(encode(line)):\n",
    "        df[i] += 1\n",
    "def tf_idf(s):\n",
    "    bow = to_bow(s)\n",
    "    return bow*torch.log((N+1)/(df+1))\n",
    "\n",
    "print(tf_idf(train_dataset[0][1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Embedding = 'map words into vectors'(not phrases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert each sentence in a list of words and pad the lists to have the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence in dataset:\n",
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Length: 69\n",
      "\n",
      "Second Sentence in dataset:\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "Length:  38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "    \n",
    "first_sentence = train_dataset[0][1]\n",
    "second_sentence = train_dataset[1][1]\n",
    "\n",
    "f_tokens = encode(first_sentence)\n",
    "s_tokens = encode(second_sentence)\n",
    "\n",
    "print(f'First Sentence in dataset:\\n{first_sentence}')\n",
    "print(\"Length:\", len(train_dataset[0][1]))\n",
    "print(f'\\nSecond Sentence in dataset:\\n{second_sentence}')\n",
    "print(\"Length: \", len(train_dataset[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([[ 121, 5501,   31,  ...,    0,    0,    0],\n",
      "        [ 199,   57,  235,  ...,    0,    0,    0],\n",
      "        [  50, 1834, 1594,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [2642,    2, 2125,  ...,    0,    0,    0],\n",
      "        [  85, 1086,   52,  ...,    0,    0,    0],\n",
      "        [   5,  215,   67,  ...,    0,    0,    0]])\n",
      "\n",
      "length of first sentence: 13\n",
      "length of second sentence: 8\n",
      "size of features: torch.Size([7613, 74])\n"
     ]
    }
   ],
   "source": [
    "labels, features = padify(train_dataset)  \n",
    "print(f'features: {features}')\n",
    "\n",
    "print(f'\\nlength of first sentence: {len(f_tokens)}')\n",
    "print(f'length of second sentence: {len(s_tokens)}')\n",
    "print(f'size of features: {features.size()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "# for w,p in w2v.most_similar('dog'):\n",
    "#     print(f\"{w} -> {p}\")\n",
    "# w2v.word_vec('play')[:20]\n",
    "# w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EmbedClassifier(torch.nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, num_class):\n",
    "#         super().__init__()\n",
    "#         self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "#         self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "#     def forward(self, text, off):\n",
    "#         x = self.embedding(text, off)\n",
    "#         return self.fc(x)\n",
    "# #Using Pre-Trained Embeddings in PyTorch\n",
    "# embed_size = len(w2v.get_vector('hello'))\n",
    "# print(f'Embedding size: {embed_size}')\n",
    "\n",
    "# net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "# print('Populating matrix, this will take some time...',end='')\n",
    "# found, not_found = 0,0\n",
    "# for i,w in enumerate(vocab.itos):\n",
    "#     try:\n",
    "#         net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "#         found+=1\n",
    "#     except:\n",
    "#         net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "#         not_found+=1\n",
    "\n",
    "# print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "# net = net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121, 5501, 31, 5, 867, 11, 27, 875, 138, 2933, 5734, 103, 50]\n",
      "(tensor([1, 1, 1, 1]), tensor([[  121,  5501,    31,     5,   867,    11,    27,   875,   138,  2933,\n",
      "          5734,   103,    50,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  199,    57,   235,   856, 21485, 21621,     2,  1376,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [   50,  1834,  1594,    10,     6,  2332,     9,   675,     6,    31,\n",
      "           134, 20367,    26,  1818,     2,    53,   401,   261,    69,  2332,\n",
      "             9,   675,  1421,    31,  1081],\n",
      "        [  837,  4809,  2888,    70,  6451,  2424,   261,  1421,     9,   100,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]))\n"
     ]
    }
   ],
   "source": [
    "def encode(x,voc=None,unk=0,tokenizer=tokenizer):\n",
    "    v = vocab if voc is None else voc\n",
    "    return [v.stoi.get(s,unk) for s in tokenizer(x)]\n",
    "\n",
    "print(encode(train_dataset[0][1]))\n",
    "# encode maps the words to the numbers\n",
    "    \n",
    "def padify(b,voc=None,tokenizer=tokenizer):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1],voc=voc,tokenizer=tokenizer) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "print(padify(train_dataset[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for epoch in range(epoch_size):\n",
    "        for labels,features in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = net(features)\n",
    "            loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss+=loss\n",
    "            _,predicted = torch.max(out,1)\n",
    "            acc+=(predicted==labels).sum()\n",
    "            count+=len(labels)\n",
    "            i+=1\n",
    "            if i%report_freq==0:\n",
    "                print(f\"{count}: acc={acc.item()/count}\")\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.59875\n",
      "6400: acc=0.64453125\n",
      "9597: acc=0.6804209648848598\n",
      "12797: acc=0.7034461201844182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.035128498365511705, 0.7143044791803494)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001, epoch_size=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,(h,c) = self.rnn(x)\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.5671875\n",
      "6400: acc=0.590625\n",
      "9597: acc=0.6270709596748985\n",
      "12797: acc=0.6549972649839806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03754812581583311, 0.6741100748719296)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LSTMClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001, epoch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3200e04dafe36a386a0a3d020faa585b26674e76377cd978f717a904722ed8bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
